{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "# from langchain_community.vectorstores import Chroma\n",
    "# from class langchain_chroma import Chroma\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "# import os\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "import json\n",
    "\n",
    "from typing import Optional, List\n",
    "from langchain.schema import Document\n",
    "\n",
    "\n",
    "#Marche avec les librairies d'environnement pour charger la clef directement depuis un environnement bashrc\n",
    "# load_dotenv()\n",
    "# os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_KEY')\n",
    "\n",
    "key = \"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\"\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "# vectordb= Chroma(persist_directory=\"./UPDATE\", embedding_function=embeddings, collection_name=\"UPDATE\")\n",
    "vectordb= Chroma(persist_directory=\"./VSTORE\", embedding_function=embeddings, collection_name=\"VSTORE\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensembles des fonctions qui assurent le fonctionnement du rag\n",
    "\n",
    "def filter_detection(question) -> dict :\n",
    "    # Définition du prompt avec indications des spécialités\n",
    "    SPECIALTIES = [\"AGRAL (Agroalimentaire)\", \"EISE (Électronique - Informatique Parcours systèmes embarqués)\", \"EI2I (Électronique - Informatique Parcours informatique industrielle)\", \"GM (Génie Mécanique)\", \"MAIN (Mathématiques appliquées et informatique)\", \"MTX (Matériaux - Chimie)\", \"ROB (Robotique)\", \"ST (Sciences de la terre : aménagement, environnement, énergie)\"]\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0, api_key = key)\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Tu es un assistant chargé d'extraire des filtres de métadonnées à partir d'une question. Voici les filtres qui nous intéressent : specialty. plusieurs informations peuvent être ajoutés dans le même filtre\"\n",
    "        \" Voici une liste des spécialités reconnues : {SPECIALTIES}.\"),\n",
    "\n",
    "        (\"human\", \"Voici la question de l'utilisateur : {question}. Retourne les informations au format JSON avec 'None' si rien ne correspond.\")\n",
    "    ])\n",
    "\n",
    "    # Chaîne de traitement\n",
    "    FilterChain = (prompt | llm | RunnablePassthrough())\n",
    "\n",
    "    response = FilterChain.invoke({\"question\": question, \"SPECIALTIES\":SPECIALTIES})\n",
    "    response = str(response.content)\n",
    "    # Convertir le texte JSON en dictionnaire Python\n",
    "    dict_answer = json.loads(response)\n",
    "    # Afficher le dictionnaire\n",
    "    print(dict_answer)\n",
    "    return dict_answer\n",
    "\n",
    "# def flatten(xss) :\n",
    "#     return [x for xs in xss for x in xs]\n",
    "\n",
    "def retrieval_single_field (query : str, _filter : str, vectordb : Chroma) :\n",
    "    docs = vectordb.similarity_search(query,k=5,filter={\"Speciality\": _filter})\n",
    "    return [doc.metadata.get(\"ID\", 'None') for doc in docs]\n",
    "\n",
    "\n",
    "def retrieval(query : str, filter : list, vectordb : Chroma) -> List[Document]:\n",
    "    # Initialiser une liste pour stocker les identifiants de documents\n",
    "    all_ids_set = set()\n",
    "    #on passe une liste de filtres (les spécialités)\n",
    "    \n",
    "    if filter :\n",
    "        for _filter in filter :\n",
    "            all_ids_set.update(retrieval_single_field(query, _filter, vectordb))\n",
    "    else:\n",
    "        # Si aucun filtre n'est fourni ou aucun résultat trouvé, retourner une liste vide\n",
    "        return []\n",
    "\n",
    "    all_ids = list(all_ids_set)\n",
    "    # print(\"all ids :\", all_ids)\n",
    "\n",
    "    return all_ids\n",
    "\n",
    "def get_document_by_id(collection, vector_id):\n",
    "    \"\"\"\n",
    "    Récupère un vecteur à partir de son ID dans une collection Chroma.\n",
    "\n",
    "    Args:\n",
    "        collection: L'objet Chroma collection où les vecteurs sont stockés.\n",
    "        vector_id: L'ID unique associé au vecteur que vous souhaitez récupérer.\n",
    "\n",
    "    Returns:\n",
    "        Le vecteur et ses métadonnées associées, ou None si l'ID n'existe pas.\n",
    "    \"\"\"\n",
    "    # Rechercher le vecteur correspondant à l'ID dans la collection\n",
    "    results = collection.get(where={\"ID\": vector_id})\n",
    "    \n",
    "    if results and results[\"ids\"]:\n",
    "        # Retourner le vecteur et ses métadonnées\n",
    "        # print(results)\n",
    "        return results\n",
    "    else:\n",
    "        # ID non trouvé\n",
    "        # print(f\"Aucun vecteur trouvé avec l'ID: {vector_id}\")\n",
    "        return None\n",
    "    \n",
    "def context_fetcher(retrieved_documents) :\n",
    "    \"\"\"Ajoute le contexte correspondant aux documents retrouvés avec les fonctions de retrieval\"\"\"\n",
    "    context = \"\"\n",
    "\n",
    "    for id in retrieved_documents :\n",
    "        doc = get_document_by_id(vectordb, id)\n",
    "        context += f\"Contenu : {doc['documents'][0]}\\n\"\n",
    "        # print(f\"Contenu : {doc['documents'][0]}\")\n",
    "\n",
    "        context += f\"Status : {doc['metadatas'][0].get('Status', 'pas de Satus')}\\n\"\n",
    "        # print(f\"Status : {doc['metadatas'][0].get('Status', 'pas de Satus')}\")\n",
    "\n",
    "        context += f\"SPE : {doc['metadatas'][0].get('Speciality', 'no spec')}\\n\"\n",
    "        # print(f\"SPE : {doc['metadatas'][0].get('Speciality', 'no spec')}\")\n",
    "\n",
    "        url = doc['metadatas'][0].get('URL', None)\n",
    "        if(url) : \n",
    "            context += f'URL : {url}\\n'\n",
    "            # print(f'URL : {url}')\n",
    "        context += \"-\" * 80 + '\\n'\n",
    "\n",
    "        # print(\"-\" * 80)\n",
    "        \n",
    "    return context\n",
    "        \n",
    "\n",
    "def total_process(question, vectordb) :\n",
    "\n",
    "    ### D'abord, extraire les filtres de la question :\n",
    "    dict_answer = filter_detection(question)\n",
    "\n",
    "    spec = dict_answer.get('specialty','')\n",
    "    \n",
    "    retrieved_documents = retrieval(question, spec, vectordb)\n",
    "    chat_model = ChatOpenAI(api_key = key)\n",
    "    output_parser = StrOutputParser()\n",
    "\n",
    "    if retrieved_documents : #Si on a retrouvé des choses correspondantes\n",
    "        context = context_fetcher(retrieved_documents)\n",
    "\n",
    "        ### 3. Construction du prompt sous forme de template avec variables\n",
    "        messages = [\n",
    "            (\"system\", \"Tu es un assistant machine. Utilise le contexte fourni pour répondre poliment à la demande.\\nsi tu ne connait pas la réponse, dit que tu ne sais pas et redirige vers des sources publiques pertinentes.\"),\n",
    "            (\"user\", \"Query: {question}\\n\\nContext:\\n{context}\")\n",
    "        ]\n",
    "\n",
    "        # Ajout des filtres utilisés\n",
    "        if spec:\n",
    "            messages.append((\"user\", f\"Voici les filtres utilisés : {', '.join(spec)}\"))\n",
    "\n",
    "        rag_prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "        ### 4. Exécution du modèle\n",
    "        \n",
    "        RAG_chain = rag_prompt | chat_model | output_parser\n",
    "\n",
    "        # Obtenir la réponse du modèle en fournissant les variables nécessaires\n",
    "        response = RAG_chain.invoke({\"question\": question, \"context\": context})\n",
    "    else:\n",
    "        # Execution d'un RAG classique en retrieval naif.\n",
    "        TEMPLATE = \"\"\"\\\n",
    "    Tu es un assistant machine, utilise le contexte fourni pour répondre poliment à la demande.\n",
    "    si tu ne connait pas la réponse, dit que tu ne sais pas et redirige vers des sources publiques pertinentes.\n",
    "\n",
    "    Query:\n",
    "    {question}\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "    \"\"\"\n",
    "        rag_prompt = ChatPromptTemplate.from_template(TEMPLATE)\n",
    "        naive_retriever = vectordb.as_retriever(search_kwargs={ \"k\" : 10})\n",
    "        setup_and_retrieval = RunnableParallel({\"question\": RunnablePassthrough(), \"context\": naive_retriever })\n",
    "        output_parser = StrOutputParser()\n",
    "\n",
    "\n",
    "        naive_retrieval_chain = setup_and_retrieval | rag_prompt | chat_model | output_parser\n",
    "        response = naive_retrieval_chain.invoke(question)\n",
    "\n",
    "    # ### 5. Retour du résultat\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'specialty': ['ST (Sciences de la terre : aménagement, environnement, énergie)']}\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "La spécialité Sciences de la Terre enseigne principalement la géologie, la prospection géophysique et la géomécanique. Si vous souhaitez en savoir plus sur les spécialités enseignées en géologie, je vous recommande de consulter des sources spécialisées telles que des sites Web universitaires ou des programmes de formation en géologie.\n"
     ]
    }
   ],
   "source": [
    "# retrieval(\"Quelles spécialitées enseignent l'informatique ?\", [\"MAIN (Mathématiques appliquées et informatique)\", \"EISE (Électronique - Informatique Parcours systèmes embarqués)\"], vectordb)\n",
    "\n",
    "Answer = total_process(\"Quelles spécialités enseignent la géologie ?\", vectordb)\n",
    "print(Answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyserini",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
