# ğŸ“‹ Preprocessing Module - Gestion des Fichiers

## ğŸ¯ Objectif

Ce module gÃ¨re la **dÃ©couverte et le mapping des fichiers** Ã  traiter :
- ğŸ—‚ï¸ **Indexation** des fichiers source
- ğŸ” **DÃ©tection des changements** (nouveaux, modifiÃ©s, supprimÃ©s)
- ğŸ“Š **Tracking du progress** et Ã©vitement des retraitements
- ğŸ—ºï¸ **Mapping** entre fichiers d'entrÃ©e et de sortie

## ğŸ“ Structure

```
preprocessing/
â”œâ”€â”€ build_map.py          # ğŸ—ï¸ Construction des mappings initiaux
â”œâ”€â”€ update_map.py         # ğŸ”„ Mise Ã  jour et dÃ©tection des changements
â”œâ”€â”€ input_maps/           # ğŸ“¥ Maps des fichiers d'entrÃ©e (gÃ©nÃ©rÃ©s)
â”œâ”€â”€ vect_maps/           # ğŸ” Maps pour vectorisation (gÃ©nÃ©rÃ©s)
â””â”€â”€ output_maps/         # ğŸ“¤ Maps des fichiers de sortie (gÃ©nÃ©rÃ©s)
```

## ğŸ—ï¸ build_map.py - Construction Initiale

### Fonctions Principales

#### `build_pdf_man_input_map()`
CrÃ©e le mapping des PDFs ajoutÃ©s manuellement.

```python
# Structure gÃ©nÃ©rÃ©e :
{
  "relative/path/document.pdf": {
    "hash": "sha256_hash",
    "path": "/absolute/path/to/document.pdf"
  }
}
```

#### `compute_file_hash(path)`
Calcule le hash SHA256 d'un fichier pour dÃ©tecter les modifications.

```python
hash_value = compute_file_hash("/path/to/file.pdf")
# Retourne : "a1b2c3d4e5f6..." ou None si erreur
```

### Workflow de Construction

```mermaid
graph TD
    A[Scan Directories] --> B[Calculate File Hashes]
    B --> C[Build Input Maps]
    C --> D[Initialize Output Maps]
    D --> E[Save JSON Maps]
```

### Types de Mappings

#### 1. PDF Manual Map
```json
{
  "MAIN/syllabus_MAIN.pdf": {
    "hash": "a1b2c3d4...",
    "path": "/srv/partage/.../syllabus_MAIN.pdf"
  },
  "MAIN/charte_bons_comportements.pdf": {
    "hash": "e5f6g7h8...",
    "path": "/srv/partage/.../charte_bons_comportements.pdf"
  }
}
```

#### 2. Data Sites Map
```json
{
  "polytech_sorbonne/json_scrapes/formation.json": {
    "hash": "x1y2z3...",
    "path": "/srv/partage/.../formation.json",
    "site": "polytech_sorbonne",
    "type": "json_scrape"
  }
}
```

## ğŸ”„ update_map.py - DÃ©tection des Changements

### Algorithme de DÃ©tection

#### 1. Scan des Fichiers Actuels
```python
def scan_current_files():
    """Scan tous les fichiers et calcule leurs hashs"""
    current_files = {}
    for file_path in scan_directories():
        current_files[file_path] = {
            "hash": compute_file_hash(file_path),
            "last_modified": os.path.getmtime(file_path)
        }
    return current_files
```

#### 2. Comparaison avec Maps Existants
```python
def detect_changes(current_files, existing_map):
    """DÃ©tecte nouveaux, modifiÃ©s, supprimÃ©s"""
    changes = {
        "new": [],      # Nouveaux fichiers
        "modified": [], # Hash diffÃ©rent
        "deleted": [],  # Plus dans current_files
        "unchanged": [] # Hash identique
    }
    
    for file_path, file_info in current_files.items():
        if file_path not in existing_map:
            changes["new"].append(file_path)
        elif file_info["hash"] != existing_map[file_path]["hash"]:
            changes["modified"].append(file_path)
        else:
            changes["unchanged"].append(file_path)
    
    for file_path in existing_map:
        if file_path not in current_files:
            changes["deleted"].append(file_path)
    
    return changes
```

### Types de Changements

#### ğŸ“ Nouveaux Fichiers
- Fichiers jamais vus auparavant
- Ajout automatique au mapping
- Marquage pour traitement

#### ğŸ”„ Fichiers ModifiÃ©s
- Hash diffÃ©rent depuis derniÃ¨re fois
- Mise Ã  jour du mapping
- Retraitement nÃ©cessaire

#### âŒ Fichiers SupprimÃ©s
- PrÃ©sents dans le mapping mais absents du disque
- Nettoyage du mapping
- Suppression des outputs correspondants

#### âœ… Fichiers InchangÃ©s
- Hash identique
- Pas de retraitement nÃ©cessaire
- Skip pour optimiser performance

## ğŸ—ºï¸ SystÃ¨me de Mapping

### ğŸ“¥ Input Maps
Indexent les fichiers sources par type :

```
input_maps/
â”œâ”€â”€ pdf_man_map.json        # PDFs manuels
â”œâ”€â”€ data_sites_map.json     # Sites scrapÃ©s
â””â”€â”€ scraped_pdfs_map.json   # PDFs scrapÃ©s
```

### ğŸ” Vect Maps
Fichiers sÃ©lectionnÃ©s pour vectorisation :

```
vect_maps/
â”œâ”€â”€ selected_files.json     # Liste des fichiers Ã  vectoriser
â””â”€â”€ vectorization_status.json  # Statut de vectorisation
```

### ğŸ“¤ Output Maps
Mapping entrÃ©e â†’ sortie :

```
output_maps/
â”œâ”€â”€ processing_results.json    # RÃ©sultats de traitement
â””â”€â”€ validation_status.json     # Statut de validation
```

## ğŸš€ Workflow Complet

### 1. Construction Initiale
```bash
# PremiÃ¨re fois : construire tous les mappings
python -c "
from preprocessing.build_map import *
build_pdf_man_input_map()
build_data_sites_input_map()
build_scraped_pdfs_input_map()
"
```

### 2. DÃ©tection des Changements
```bash
# RÃ©gulier : dÃ©tecter les changements
python -c "
from preprocessing.update_map import update_all_maps
changes = update_all_maps()
print(f'Nouveaux: {len(changes[\"new\"])}')
print(f'ModifiÃ©s: {len(changes[\"modified\"])}')
"
```

### 3. Traitement Intelligent
```python
# Dans main.py
from preprocessing.update_map import get_files_to_process

# Ne traiter que les fichiers nouveaux/modifiÃ©s
files_to_process = get_files_to_process()
for file_path in files_to_process:
    process_file(file_path)
```

## ğŸ“Š Optimisations

### 1. Cache des Hashs
```python
# Ã‰viter recalcul des hashs inchangÃ©s
hash_cache = {}

def compute_file_hash_cached(path: str) -> str:
    mtime = os.path.getmtime(path)
    cache_key = f"{path}:{mtime}"
    
    if cache_key in hash_cache:
        return hash_cache[cache_key]
    
    hash_value = compute_file_hash(path)
    hash_cache[cache_key] = hash_value
    return hash_value
```

### 2. Scan ParallÃ¨le
```python
from concurrent.futures import ThreadPoolExecutor

def parallel_file_scan(directories):
    """Scan parallÃ¨le de multiples rÃ©pertoires"""
    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = [
            executor.submit(scan_directory, dir_path)
            for dir_path in directories
        ]
        
        results = {}
        for future in futures:
            results.update(future.result())
        
        return results
```

### 3. Incremental Updates
```python
def incremental_update(last_scan_time):
    """Met Ã  jour seulement les fichiers modifiÃ©s depuis last_scan_time"""
    current_time = time.time()
    
    for file_path in scan_directories():
        if os.path.getmtime(file_path) > last_scan_time:
            update_file_mapping(file_path)
    
    save_last_scan_time(current_time)
```

## ğŸ› ï¸ Utilisation

### Workflow Standard
```python
from preprocessing.build_map import build_all_maps
from preprocessing.update_map import update_all_maps

# Construction initiale (premiÃ¨re fois)
if not maps_exist():
    build_all_maps()

# DÃ©tection des changements
changes = update_all_maps()

# Traitement des changements
if changes["new"] or changes["modified"]:
    process_changed_files(changes)
```

### Integration avec Main Pipeline
```python
# main.py
def main():
    # 1. DÃ©tecter les changements
    changes = update_all_maps()
    
    # 2. Traiter seulement les changements
    files_to_process = changes["new"] + changes["modified"]
    
    if not files_to_process:
        print("Aucun fichier Ã  traiter (pas de changements)")
        return
    
    # 3. Pipeline normal sur fichiers modifiÃ©s
    process_files(files_to_process)
```

## ğŸ“Š MÃ©triques et Monitoring

### Statistiques Utiles
```python
def generate_stats():
    """GÃ©nÃ¨re des statistiques sur les mappings"""
    stats = {
        "total_files": count_total_files(),
        "by_type": count_files_by_type(),
        "by_status": count_files_by_status(),
        "storage_size": calculate_total_size(),
        "last_update": get_last_update_time()
    }
    return stats
```

### Dashboard Simple
```python
def print_dashboard():
    """Affiche un dashboard des mappings"""
    stats = generate_stats()
    
    print("ğŸ“Š Preprocessing Dashboard")
    print(f"Total Files: {stats['total_files']}")
    print(f"PDFs Manual: {stats['by_type']['pdf_manual']}")
    print(f"JSONs Scraped: {stats['by_type']['json_scraped']}")
    print(f"Last Update: {stats['last_update']}")
```

## ğŸ¯ Bonnes Pratiques

1. **ğŸ”„ Incremental** : Toujours privilÃ©gier les mises Ã  jour incrÃ©mentales
2. **ğŸ›¡ï¸ Robustesse** : GÃ©rer les erreurs de lecture/Ã©criture de fichiers
3. **ğŸ“Š Monitoring** : Logger les changements dÃ©tectÃ©s
4. **âš¡ Performance** : Utiliser le parallÃ©lisme pour gros volumes
5. **ğŸ§¹ Cleanup** : Nettoyer rÃ©guliÃ¨rement les mappings obsolÃ¨tes
