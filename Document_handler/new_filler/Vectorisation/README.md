# ğŸ” Vectorisation Module - PrÃ©paration RAG

## ğŸ¯ Objectif

Ce module prÃ©pare les documents normalisÃ©s pour la **recherche RAG** en :
- âœ‚ï¸ **DÃ©coupant** intelligemment les documents
- ğŸ”— **Enrichissant** les mÃ©tadonnÃ©es pour la recherche
- ğŸ—ƒï¸ **Stockant** dans ChromaDB pour l'indexation vectorielle
- ğŸš« **DÃ©duplicant** pour Ã©viter la pollution de la base

## ğŸ“ Structure

```
Vectorisation/
â”œâ”€â”€ vectorisation_chunk.py    # ğŸš€ Pipeline principal de vectorisation
â””â”€â”€ vectorstore_Syllabus/     # ğŸ—ƒï¸ Base ChromaDB (gÃ©nÃ©rÃ©e)
    â”œâ”€â”€ chroma.sqlite3
    â””â”€â”€ ... (mÃ©tadonnÃ©es ChromaDB)
```

## ğŸš€ vectorisation_chunk.py - Pipeline Principal

### Fonctions Principales

#### `load_normalized_docs()`
Charge tous les documents JSON normalisÃ©s depuis le dossier validÃ©.

```python
docs = load_normalized_docs()
# Retourne : List[Dict] - Documents JSON normalisÃ©s
```

#### `load_syllabus_docs()`
Charge spÃ©cifiquement les documents syllabus (pattern `syllabus*.json`).

```python
syllabus_docs = load_syllabus_docs()
# Retourne : List[Dict] - Documents syllabus uniquement
```

#### `convert_to_documents(raw_docs)`
Convertit les documents JSON en objets LangChain Document avec chunking.

```python
langchain_docs = convert_to_documents(normalized_docs)
# Retourne : List[Document] - PrÃªts pour ChromaDB
```

#### `ensure_polytech_structure(doc)`
Normalise la structure pour respecter le schÃ©ma Polytech.

```python
normalized = ensure_polytech_structure(raw_doc)
# Garantit : document_type, metadata, source, content, tags
```

### Pipeline de Transformation

```mermaid
graph TD
    A[Documents JSON NormalisÃ©s] --> B[VÃ©rification Structure]
    B --> C[Aplatissement MÃ©tadonnÃ©es]
    C --> D[Chunking Intelligent]
    D --> E[Enrichissement MÃ©tadonnÃ©es]
    E --> F[ChromaDB Storage]
    
    G[Documents Syllabus] --> H[Chunking SpÃ©cialisÃ©]
    H --> I[MÃ©tadonnÃ©es PÃ©dagogiques]
    I --> F
```

## âœ‚ï¸ StratÃ©gie de Chunking

### Configuration par DÃ©faut
```python
splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,        # Taille optimale pour embeddings
    chunk_overlap=50       # PrÃ©servation du contexte
)
```

### Chunking Intelligent
1. **Respect du contenu** : Ã‰vite de couper au milieu des phrases
2. **Taille optimisÃ©e** : 500 caractÃ¨res = ~100 tokens (optimal pour embeddings)
3. **Chevauchement** : 50 caractÃ¨res pour maintenir le contexte
4. **Filtrage** : Ignore les chunks vides ou trop courts

## ğŸ·ï¸ Enrichissement des MÃ©tadonnÃ©es

### Aplatissement Automatique
Les mÃ©tadonnÃ©es complexes sont aplaties pour ChromaDB :

```python
# Structure originale
{
  "metadata": {"title": "Cours IA"},
  "source": {"site": "polytech", "url": "..."}
}

# Structure aplatie
{
  "metadata.title": "Cours IA",
  "source.site": "polytech",
  "source.url": "..."
}
```

### MÃ©tadonnÃ©es Enrichies par Chunk
Chaque chunk hÃ©rite de :
- **MÃ©tadonnÃ©es du document parent**
- **Informations de source** (chemin, site, catÃ©gorie)
- **Tags automatiques** (gÃ©nÃ©rÃ©s par IA)
- **Type de document** (cours, administratif, etc.)
- **MÃ©tadonnÃ©es spÃ©cialisÃ©es** (niveau, spÃ©cialitÃ©, etc.)

## ğŸ—ƒï¸ Stockage ChromaDB

### Configuration
```python
VECTORSTORE_DIR = Path(__file__).parent / "vectorstore_Syllabus"
embeddings = OpenAIEmbeddings()
db = Chroma(persist_directory=str(VECTORSTORE_DIR), embedding_function=embeddings)
```

### Structure de la Base
- **Documents** : Chunks de contenu
- **MÃ©tadonnÃ©es** : Informations structurÃ©es pour filtrage
- **Embeddings** : Vecteurs OpenAI pour recherche sÃ©mantique
- **IDs** : Identifiants uniques par chunk

## ğŸš« Gestion des Doublons

### ProblÃ¨me IdentifiÃ©
Le systÃ¨me peut crÃ©er des doublons lors de :
- Retraitement du mÃªme document
- Sources multiples pour le mÃªme contenu
- Erreurs de pipeline

### Solution RecommandÃ©e
```python
def deduplicate_chunks(chunks):
    """DÃ©duplication par hash de contenu"""
    seen = set()
    unique_chunks = []
    
    for chunk in chunks:
        content_hash = hashlib.md5(chunk.page_content.encode()).hexdigest()
        if content_hash not in seen:
            seen.add(content_hash)
            unique_chunks.append(chunk)
    
    return unique_chunks
```

## ğŸ” Recherche et Filtrage

### Exemple de Recherche
```python
# Recherche sÃ©mantique
results = db.similarity_search(
    "table des matiÃ¨res semestre 6",
    k=5
)

# Recherche avec filtre
results = db.similarity_search(
    "cours mathematiques",
    k=10,
    filter={"document_type": "cours", "metadata.niveau": "Semestre 6"}
)
```

### Filtres Disponibles
- **document_type** : cours, administratif, vie_etudiante, etc.
- **metadata.niveau** : Semestre 5, Semestre 6, etc.
- **metadata.specialite** : MAIN, GI, etc.
- **source.site** : MAIN, polytech_sorbonne, etc.
- **tags** : mots-clÃ©s automatiques

## ğŸ“Š MÃ©triques de Performance

### Statistiques Typiques
- **Documents traitÃ©s** : ~300-500 par batch
- **Chunks gÃ©nÃ©rÃ©s** : ~3-5 par document
- **Taille moyenne chunk** : 350-450 caractÃ¨res
- **Temps de traitement** : ~2-3s par document

### Optimisations Possibles
1. **Cache des embeddings** pour Ã©viter recalcul
2. **Batch processing** pour rÃ©duire les appels API
3. **Indexation parallÃ¨le** pour gros volumes
4. **Compression des mÃ©tadonnÃ©es** pour optimiser l'espace

## ğŸ› ï¸ Utilisation

### Pipeline Complet
```python
from Vectorisation.vectorisation_chunk import (
    load_normalized_docs, 
    convert_to_documents
)

# 1. Charger les documents
docs = load_normalized_docs()

# 2. Convertir en chunks LangChain
chunks = convert_to_documents(docs)

# 3. Stocker dans ChromaDB
db = Chroma.from_documents(
    chunks, 
    embeddings, 
    persist_directory=str(VECTORSTORE_DIR)
)
```

### Recherche Simple
```python
# Rechercher des TOCs
results = db.similarity_search("table des matiÃ¨res", k=5)
for doc in results:
    print(f"Contenu: {doc.page_content}")
    print(f"Source: {doc.metadata.get('source.site')}")
```

## ğŸ¯ Recommandations

1. **ğŸš€ Performance** : Traiter par batch de 50-100 documents
2. **ğŸ” QualitÃ©** : Valider que les chunks prÃ©servent le sens
3. **ğŸ“Š Monitoring** : Tracker les mÃ©triques de chunking
4. **ğŸš« DÃ©duplication** : ImplÃ©menter avant mise en production
