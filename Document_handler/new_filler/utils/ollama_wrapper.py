"""
Module d'interrogation de mod√®les de langage (LLM).

Ce module fournit une interface unifi√©e pour envoyer des prompts √† des mod√®les
locaux (via Ollama) ou distants (via l'API OpenAI).

Fonctionnalit√©s :
- Routage automatique du prompt vers le bon moteur (OpenAI ou Ollama)
- Gestion des erreurs et des timeouts
- Appels via subprocess ou API OpenAI

Exemples d'utilisation :
    response = ask_model("Explique la gravit√© en 2 phrases.", engine="openai")
    print(response)

Configuration requise :
- D√©finir les variables `OLLAMA_MODEL`, `OPENAI_MODEL`, et `OPENAI_API_KEY` dans le fichier config

"""
import subprocess
import openai
import os
from ..config import OLLAMA_MODEL, OPENAI_MODEL, OPENAI_API_KEY

def ask_model(prompt: str, engine: str = "openai") -> str:
    """
    Envoie un prompt au mod√®le sp√©cifi√© (OpenAI ou Ollama).

    Args:
        prompt (str): Le prompt √† envoyer au mod√®le.
        engine (str): Le moteur √† utiliser, "openai" ou "ollama".

    Returns:
        str: La r√©ponse g√©n√©r√©e par le mod√®le.

    Raises:
        ValueError: Si le moteur sp√©cifi√© est inconnu.
    """
    if engine == "ollama":
        return ask_ollama(prompt)
    elif engine == "openai":
        return ask_openai(prompt)
    else:
        raise ValueError(f"‚ùå Unknown engine: {engine}")

def ask_ollama(prompt: str) -> str:
    """
    Interroge un mod√®le local via Ollama.

    Args:
        prompt (str): Le prompt √† envoyer.

    Returns:
        str: La r√©ponse du mod√®le Ollama.

    Raises:
        RuntimeError: Si Ollama √©choue ou ne retourne rien.
    """
    try:
        result = subprocess.run(
            ["ollama", "run", OLLAMA_MODEL],
            input=prompt.encode("utf-8"),
            capture_output=True,
            timeout=60
        )
        output = result.stdout.decode("utf-8").strip()
        if not output:
            raise RuntimeError("Ollama n‚Äôa rien retourn√©.")
        return output
    except subprocess.TimeoutExpired:
        raise RuntimeError("‚è±Ô∏è Timeout pendant l'appel √† Ollama.")
    except Exception as e:
        raise RuntimeError(f"üí• Erreur Ollama: {e}")

def ask_openai(prompt: str) -> str:
    """
    Interroge un mod√®le distant via l'API OpenAI.

    Args:
        prompt (str): Le prompt √† envoyer.

    Returns:
        str: La r√©ponse du mod√®le OpenAI.

    Raises:
        RuntimeError: Si la cl√© API est absente ou si la requ√™te √©choue.
    """
    if not OPENAI_API_KEY:
        raise RuntimeError("üîê Cl√© API OpenAI manquante (OPENAI_API_KEY).")

    client = openai.OpenAI(api_key=OPENAI_API_KEY)

    try:
        response = client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[
                {"role": "user", "content": prompt}
            ],
            temperature=0.3
        )
        return response.choices[0].message.content.strip()

    except Exception as e:
        raise RuntimeError(f"üí• Erreur OpenAI: {e}")
